As an amateur astrophotographer, I find deep-sky objects to be some of the most rewarding targets to capture. They display a stunning range of colors and structures that exemplify the beauty of the cosmos. Their characteristics also demand highly unique imaging and processing techniques, presenting some interesting challenges that we'll be discussing today.

So&hellip; what *is* a deep sky object, exactly? Broadly speaking, "DSO" encompasses everything outside of the Solar System (besides individual stars). This category includes objects such as star clusters, nebulae, and galaxies. DSOs vary wildly in apparent size, but they generally have one thing in common: they are very faint, meaning that very long exposures are necessary to fully bring out their details.

The requirement for long exposure time brings us to our first major obstacle: objects in the sky appear to be in constant motion due to Earth's rotation. If we don't account for this, our images of the target will appear hopelessly smeared due to motion blur. The solution is to rotate our imaging setup along the *polar axis* in the opposite direction of Earth's rotation, which can be accomplished with the help of an [equatorial mount](https://en.wikipedia.org/wiki/Equatorial_mount). This will make the sky appear stationary to the camera. 

Unfortunately, tiny deviations in the mount's alignment are inevitable. As we push our exposure length longer and longer, these small errors will accumulate into trailing, which is unacceptable. This sets an upper limit on the exposure time of an individual frame. To overcome this limitation, we must combine ("stack") multiple exposures into a final image. Using many exposures also allows us to detect and exclude satellite trails from the final image.

Say we've acquired the data&mdash;what now? Well, this is where the fun begins.

# Calibration

![comparison of ccd and cmos sensors](ccd-vs-cmos.jpg)

Modern cameras use either CCD or CMOS sensors to detect light and produce a digital readout. While these technologies have many important differences, they function on similar principles: photosites (pixels) in the sensor are struck by photons, some of which end up producing electrons. These electrons accumulate within the photosites over the span of the exposure. At the end, the charges are converted to a voltage and then digitized by an **analog-to-digital converter (ADC)**. The amount of amplification applied is known as the **gain**, which is primarily determined by the ISO setting.

CCD/CMOS systems mainly suffer from two sources of noise:

- **dark noise**, which is the product of charges spontaneously generated within the sensor
- **read noise**, which is generated by the amplifiers and ADC

There is one additional source of noise in our images: **shot noise**. It stems from the fact that light is quantum, meaning that rather than being radiated continuously, it arrives in discrete packets of energy called photons. Over a finite timespan, there is always some uncertainty in the exact number of photons received from the target, giving rise to shot noise.

<canvas id="shot-noise-demo" style="border: 1px solid #ccc" width="600" height="300"></canvas><br>
<label for="exposure-time">Exposure time: </label><input type="range" min="0.5" max="4" step="0.5" value="1" id="exposure-time"><span id="exposure-time-text">

Check out this demo, which simulates randomly emitted particles falling onto an ideal detector. Try playing around with the exposure time, and see how it affects the signal-to-noise ratio (SNR).

<script src="shot-noise.js"></script>
 
<aside>

Shot noise doesn't require any treatment in the calibration process, because it's an inherent part of the signal. However, it does have some interesting properties that can help us characterize the sensor.

For our purposes, we treat photon emission events as occuring independently of each other at a constant mean rate. This is a Poisson process, meaning that the number of events observed over a fixed time interval is given by the [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution). When $\lambda$ is large, the Poisson distribution is closely approximated by a normal distribution with $\mu = \sigma^2 = \lambda$. Thus, we can say that the variance of the number of electrons is equal to the average number of electrons.

Of course, our RAW files don't tell us how many electrons were produced within the pixels. Instead, they contain arbitrary integer quantities representing the output of the ADC. It doesn't make sense to assign a physical unit to these values, so for the sake of clarity we measure these values in **analog-to-digital units (ADUs)**. There is some factor that gives the electrons per ADU, which we call the **gain**.

The signal statistics in ADU and electrons are related by:

$$\mu_\text{ADU} = \frac{\mu_\mathrm{e^-}}{\mathrm{gain}}$$

$$\sigma_\text{ADU} = \frac{\sigma_\mathrm{e^-}}{\mathrm{gain}}$$

We know that $\sigma_\mathrm{e^-} = \sqrt{\mu_\mathrm{e^-}}$ due to shot noise, allowing us to solve for gain:

$$\mathrm{gain} = \frac{\mu_\mathrm{ADU}}{\sigma^2_\mathrm{ADU}}$$

</aside>

## Read Noise

## Dark Noise

Even in the absence of light, the photosites within a sensor will still produce some electrons due to thermal effects. This unwanted signal is known as **dark current**. Different pixels produce dark current at different rates, meaning that we much estimate the dark current level on a pixel-by-pixel basis. (Of course, dark current is also subject to shot noise, meaning that it varies randomly.)

To estimate the dark current signal, we'll take some exposures with the same length and ISO as our light frames, but with the lens cap on. These are called *dark frames*. Multiple dark frames are necessary to reduce the effect of random variation; we can combine them using {median/average?}. We also need to subtract the master bias frame to eliminate read noise.

<aside>

Dark noise levels depend on temperature. 

</aside>

## Flat-fielding

# Alignment

# Stacking

# Post-processing

