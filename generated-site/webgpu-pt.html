<!DOCTYPE html><html lang="en" class="serif"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Pathtracing with WebGPU</title><meta property="og:title" content="Pathtracing with WebGPU"><meta property="og:type" content="website"><meta property="og:description" content="WebGPU is the next-gen GPU API for the web, allowing us to unleash the number-crunching capacity of our graphics cards. Let's use it to write a physically-based renderer."><meta name="description" content="WebGPU is the next-gen GPU API for the web, allowing us to unleash the number-crunching capacity of our graphics cards. Let's use it to write a physically-based renderer."><link rel="stylesheet" href="static/stylesheets/main.css"><link rel="stylesheet" href="static/stylesheets/highlight-style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script defer src="static/scripts/pt.js"></script><script defer src="static/scripts/ui.js"></script><link rel="icon" type="image/png" sizes="16x16" href="static/images/favicon.png"><link rel="canonical" href="https://blog.bithole.dev/webgpu-pt.html"></head><body><header><a href="/"><img src="static/images/banner.jpg" alt="blog banner"></a></header><main><p id="date" class="date">December 31, 1969</p><h1 style="margin-top: 0">Pathtracing with WebGPU</h1><nav><div id="contents"><p>Table of Contents</p><a href="#a-pbr-crash-course"><p>A PBR Crash Course</p></a><a href="#solving-the-rendering-equation"><p>Solving the Rendering Equation</p></a></div><button id="show-toc">&#9776; Contents</button></nav><h1 id="a-pbr-crash-course">A PBR Crash Course <a class="section-link" href="#a-pbr-crash-course">&sect;</a></h1><p><strong>TODO: intro</strong></p>
<p style="text-align: center"><b>What determines how a material looks?</b></p>

<p>We perceive materials based on how they scatter light. Thus, our material model needs to explain what happens to a ray of light when it hits a material.</p>
<p>There are two primary interactions which occur:</p>
<ul>
<li>Absorption: some frequencies of light are absorbed more strongly than others, creating color.</li>
<li>Scattering: how light bounces off the material will determine its texture. Consider the two most extreme examples: a perfect mirror will always reflect light in the same direction, while a painted surface with matte finish will scatter light rays in random directions.</li>
</ul>
<p>For our purposes, it is enough to model absorption by multiplying the spectrum of the incoming light against a set of coefficients. The scattering of light is a random process, so we can characterize it using its distribution. (This assumes that the scattering events are independent; I can&#39;t think  of any physical situation where this wouldn&#39;t be the case). The distribution function looks like this:</p>
<div class="indented">

<p>Let $\text{BRDF}(\omega_i, \omega_o)$ be the proportion of light coming from direction $\omega_i$ reflected in direction $\omega_o$.</p>
</div>

<p>BRDF stands for Bidirectional Reflectance Distribution Function. One important property of BRDFs which their name betrays is reciprocity: $\text{BRDF}(\omega_i, \omega_o)$ = $\text{BRDF}(\omega_o, \omega_i)$. 
Intuitively, this says that reflection is the same in both directions. </p>
<p><strong>TODO: brdf visualization</strong></p>
<p style="text-align: center"><b>How do we simulate how light moves through a scene?</b></p>

<p>Depending on the level of accuracy you demand, the answer to this question might end up encompassing the entirety of physics. However, for the purposes of 3D rendering, it&#39;s safe to make some simplifying assumptions about the behavior of light. We treat light as a series of rays which always travel in straight lines, neglecting wave effects like diffraction and interference. By sacrificing these behaviors (which aren&#39;t noticeable in most scenes), we can use <strong>raytracing</strong> to simulate light transport.</p>
<p>With that, we can write an expression for the light radiated by a point in terms of sources of illumination in the rest of the scene; this equation basically dictates how to shade the scene.</p>
<div class="indented">

<p>Let $L_o(x, \omega_o)$ be the intensity of light radiated from point $x$ towards direction $\omega_o$, $L_i(x, \omega_i)$ be the intensity of light hitting point $x$ from direction $\omega_i$, and $L_e(x)$ be the light emitted from point $x$.</p>
</div>

<p>$$L_o(x, \omega_o) = L_e(x) + \int_\Omega \text{BRDF}(\omega_i, \omega_o) L_i(x, \omega_i) (\omega_i \cdot n) ,d\omega_i$$</p>
<p>Wow, what a mouthful! Don&#39;t be scared; we can actually pick this apart piece-by-piece. Let&#39;s add a little bit of color: </p>
<p>$$\textcolor{red}{L_o(x, \omega_o)} = \textcolor{green}{L_e(x)},+\int_{\Omega} \textcolor{blue}{\text{BRDF}(\omega_i, \omega_o) L_i(x, \omega_i)}\textcolor{orange}{(\omega_i \cdot n)},d\omega_i$$</p>
<p>This equation basically boils down to...</p>
<div class="indented">

<p>The <span style="color: #ff0000">light emitted by point $x$</span> is equal to the <span style="color: #009B55">light emitted from point $x$</span> plus the sum of the <span style="color: #0000ff">light hitting point $x$, multiplied by the likelihood of light being reflected towards the viewer</span>.</p>
</div>

<p>But wait, what&#39;s that last term, $\omega_i \cdot  n$? For starters, $n$ is the normal vector at point $x$. That basically means it&#39;s the vector perpendicular to the surface at point $x$. Since both $n$ and $\omega_i$ are unit vectors, their dot product is simply equal to the cosine of the angle between them. You will often see this written as $\cos\theta_i$.</p>
<p>If you want a rigorous explanation of this term, I would highly recommend that you read the section about radiometry (specifically, the definition of radiance, which is the quantity given by our equation) in <a href="https://pbr-book.org/3ed-2018/Color_and_Radiometry/Radiometry"><em>Physically Based Rendering</em></a>. For now, my quick-and-dirty explanation will suffice.</p>
<p>Basically, the product of the BRDF and the incoming radiance tells us how much energy is going towards the viewer. However, as the surface becomes more tilted relative to the viewer, the apparent area it takes up in our view increases. Since the energy is spread out over a wider area, the radiance is lower. This phenomenon is known as <a href="https://en.wikipedia.org/wiki/Lambert%27s_cosine_law">Lambert&#39;s law</a>.</p>
<p>Think about what happens if you shine a light against a wall. It will appear brightest when you shine it directly perpendicular to the wall; in this case, $\theta_i = 0\degree$, so $\cos\theta_i = 1$. As you tilt the flashlight, the brightness of the wall decreases. Eventually, the beam will be parallel with the wall, at which point $\theta_i = 90\degree$ and $\cos\theta_i = 0$.</p>
<p>This equation, put together, is known as the <strong>rendering equation</strong>. Solving it is the focus of physically-based rendering.</p>
<aside>

<p>For the sake of simplicity, we assume that all light sources emit at equal intensity in all directions; that is, they are <em>isotropic</em>. Thus, $L_e$ is expressed only in terms of $x$.</p>
</aside>

<h2 id="solving-the-rendering-equation">Solving the Rendering Equation <a class="section-link" href="#solving-the-rendering-equation">&sect;</a></h2><p>The job of our renderer, in a nutshell, is to solve the rendering equation. Unfortunately, most of the integration methods we normally use are useless here because the rendering equation is very high-dimensional; in fact, unless we place some limits on the rendering equation, the number of dimensions is <strong>infinite</strong>! Thankfully, one class of methods stands out for very high dimension problems: <a href="https://en.wikipedia.org/wiki/Monte_Carlo_integration">Monte Carlo integration</a>.</p>
<p>The idea behind Monte Carlo integration is simple: if we evaluate the function for random points on its domain, the average will approach the integral of the function. Essentially:</p>
<div class="indented">

<p>Let $x_0, x_1 \ldots x_n$ be uniformly distributed random numbers in the range $[a, b]$. </p>
</div>

<p>$$\lim_{n\to\infty} \frac{1}{n} \sum_{i=0}^n f(x_i) = \int_{a}^{b} f(x) ,dx$$</p>
<p>In some cases, our samples might not be uniformly distributed. In that case, we can simply weight each sample by the PDF of each sample.</p>
<p>$$\lim_{n\to\infty} \frac{1}{n} \sum_{i=0}^n \frac{f(x_i)}{\text{pdf}(x_i)} = \int_{a}^{b} f(x) ,dx$$</p>
<p>For the statistically inclined folk among the audience, this method of estimating an integral is <strong>unbiased</strong>; that is, given an infinite number of iterations, it will always converge onto the &quot;correct&quot; answer.</p>
<p>Here&#39;s a very basic demonstration of a Monte Carlo method, where we pick random points and count how many fall within a circle to estimate the value of pi.</p>
<p><canvas id="pi-mc" width="256" height="256"></canvas></p>
<p><button id="pi-button">Run</button> <span id="pi-stats"></span></p>
<p>Notice how as the number of points increases, the error progressively decreases. It can be shown that a Monte Carlo integrator will converge on the answer at a rate of $\frac{1}{\sqrt{n}}$, regardless of dimension. While this rate is unbearably slow compared to other methods when applied to 1D integration, as the number of dimensions grows, Monte Carlo integration gains a significant edge.</p>
<img id="img-view" style="display: none"><h1>Comments</h1><noscript><b>Please enable Javascript to view the comments on this post.</b></noscript><script src="https://utteranc.es/client.js" crossorigin="anonymous" repo="adrian154/blog" issue-term="title" label="blog-post-comments" theme="github-light"></script></main><footer><p>&copy; 2022 <a href="https://bithole.dev/">Adrian Zhang</a> &bull; <a href="rss.xml">rss</a> &bull; <a href="https://github.com/adrian154/blog">source</a> &bull; <a href="https://creativecommons.org/licenses/by-sa/3.0/legalcode">CC BY-SA 3.0</a></p></footer></body></html>